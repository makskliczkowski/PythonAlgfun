{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "df17d623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import collections\n",
    "import numpy as np\n",
    "from nltk.book import *\n",
    "from stemming.porter2 import stem\n",
    "from string import *\n",
    "from itertools import combinations, product, permutations\n",
    "import sys\n",
    "import hashlib\n",
    "import random\n",
    "import mmh3\n",
    "from datetime import datetime\n",
    "random.seed(datetime.now())\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a5a278",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "733378f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_dist(a,b):\n",
    "    # take smaller to handle unequal len\n",
    "    length = min(len(a),len(b))\n",
    "    #if length > len(b): length = len(b)\n",
    "    return sum(e1 != e2 for (e1,e2) in zip (a[:length],b[:length]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "be982048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_sim_bags(b1,b2):\n",
    "    if(sum(b1.values())*sum(b2.values()) == 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return sum((b1 & b2 ).values())/sum((b2 | b1).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3f810ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_words_from_text(text):\n",
    "    words = list(filter(None, [word.lower().translate(str.maketrans('', '', punctuation))\n",
    "                           for line in text\n",
    "                           for word in line.split()\n",
    "                           ]))\n",
    "    words = [stem(word) for word in words if word not in stopwords]\n",
    "    return collections.Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "bd503a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_edit_dist(a : str,b: str):\n",
    "    if(len(a)!= 0 and len(b) !=0):\n",
    "        return nltk.edit_distance(a,b)/(len(a)+len(b))\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "b0993697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_words_cut(text, cutoff = 8):\n",
    "    words = set(filter(None, [word.lower().translate(str.maketrans('', '', punctuation))\n",
    "                               for line in text\n",
    "                               for word in line.split()\n",
    "                               if len(word) <= cutoff]))\n",
    "    words = {stem(word) for word in words if word not in stopwords}\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "d9cc35ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_shingle_set(s, k):\n",
    "    return {''.join(tup) for tup in nltk.ngrams(list(s),k)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9730644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#int min(int x, int y) \n",
    "#{ \n",
    "#    return y ^ ((x ^ y) & -(x < y)); \n",
    "#} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "d6af2a84",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-327-7d8c3ebca595>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-327-7d8c3ebca595>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    def max(x, y)\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Function to find maximum of x and y\n",
    "#def max(x, y) \n",
    "#{ \n",
    "    #return x ^ ((x ^ y) & -(x < y));  \n",
    "#}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0b5d38",
   "metadata": {},
   "source": [
    "#### 1) For a given bitstring b list all bitstrings bâ€™, such that the Hamming distance between b and b' equal to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "748e59b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['111100010', '110110010']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hamming(b, ls, equals):\n",
    "    # takes the list of bit strings, to handle exceptions it implicitly converts to true or false\n",
    "    # treating elements by making them equal. Equals says how many should it equal\n",
    "    return [ a for a in ls if hamming_dist(a,b) == 1]\n",
    "ls = [\"011101110\", \"111100010\",\"110110010\",\"0110000010\",\"010011110\",\"011101010110\"]\n",
    "b = \"111110010\"\n",
    "hamming(b,ls,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b492bda",
   "metadata": {},
   "source": [
    "#### 2) Construct a function that returns a Jaccard similarity for two sets. Beware that this function needs to check if at least one of the sets is nonempty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9c85e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4444444444444444"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def second(A,B):\n",
    "    if(len(A)*len(B) == 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return len(A.intersection(B))/len(A.union(B))\n",
    "A = {1,3,4,61,1, \"hej\"}\n",
    "B = {1,3,4,5,32,7,\"hej\", \"pa\"}\n",
    "second(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4072be",
   "metadata": {},
   "source": [
    "#### 3) Construct a function that computes Jaccard similarity for two strings treated as bags of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8913a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def third(strA,strB):\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    if(len(strA)*len(strB) == 0):\n",
    "        return 0\n",
    "    else:\n",
    "        A = collections.Counter(tokenizer.tokenize(strA.lower()))\n",
    "        B = collections.Counter(tokenizer.tokenize(strB.lower()))\n",
    "        #print(A)\n",
    "        return sum((A & B ).values())/sum((A | B).values())\n",
    "                                \n",
    "Astr = \"hej, jestem pierwszym zdaniem pierwszym.\"\n",
    "Bstr = \"a ja drugim zdaniem, zdaniem pierwszym. pierwszym\"\n",
    "third(Astr,Bstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1420711a",
   "metadata": {},
   "source": [
    "#### 4) (use NLTK) List all words in text1 with edit distance from the word dog smaller than 4. Hint: you can safely reject all long words without computations (why?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6188b5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['dig', 'dug', 'log', 'fog', 'do', 'dong', 'don', 'dogs', 'sog']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fourth(directory,word = \"dog\", distance = 4, transposition = False):\n",
    "    # directory of text file to be analysed, distance to be used as a condition, allow edit with \n",
    "    # transpositions as Levenshtein did\n",
    "    # we can reject words longer than len(word) + distance and len(word) - distance because of the\n",
    "    # impossibility to transform them into one another\n",
    "    with open(directory) as file:\n",
    "        #print(nltk.edit_distance(\"shine\", \"dine\"))\n",
    "        word_len = len(word)\n",
    "        return {a for line in file for a in set(tokenizer.tokenize(line.lower())) if ((word_len - distance) <= len(a) <= (word_len + distance)) if (nltk.edit_distance(word,a,transpositions) == distance)}\n",
    "def fourth_with_text(text = text1,word = \"dog\", dist = 4):\n",
    "    # text to be analysed, distance to be used as a condition, allow edit with \n",
    "    # transpositions as Levenshtein did\n",
    "    # we can reject words longer than len(word) + distance and len(word) - distance because of the\n",
    "    # impossibility to transform them into one another\n",
    "        #print(nltk.edit_distance(\"shine\", \"dine\"))\n",
    "    word_len = len(word)\n",
    "    words = set(filter(None, [word.lower().translate(str.maketrans('', '', punctuation))\n",
    "                           for line in text\n",
    "                           for word in line.split()\n",
    "                           if ((word_len - dist) <= len(word) <= (word_len + dist))\n",
    "                           ]))\n",
    "    return [a for a in words if (nltk.edit_distance(word,a) == dist)]\n",
    "#fourth(\"./books/BookAboutGoats.txt\", \"dog\", 4,False)\n",
    "a = fourth_with_text(dist = 1)\n",
    "#print(nltk.edit_distance(\"dog\", \"lock\"))\n",
    "print(len(a))\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30282b43",
   "metadata": {},
   "source": [
    "#### 5) (use NLTK) Let text1 - text9 be bags of words. Compute similarity between all pairs of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "10eb3e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard similarity between text1->text2 is 0.25880168699000644\n",
      "Jaccard similarity between text1->text3 is 0.09161654866732645\n",
      "Jaccard similarity between text1->text4 is 0.2259924160801489\n",
      "Jaccard similarity between text1->text5 is 0.07560581583198707\n",
      "Jaccard similarity between text1->text6 is 0.037187020274204646\n",
      "Jaccard similarity between text1->text7 is 0.1462570078047708\n",
      "Jaccard similarity between text1->text8 is 0.013818097422942684\n",
      "Jaccard similarity between text1->text9 is 0.19548115859792112\n",
      "Jaccard similarity between text2->text3 is 0.11810597868067357\n",
      "Jaccard similarity between text2->text4 is 0.27273410410829496\n",
      "Jaccard similarity between text2->text5 is 0.10354935497697827\n",
      "Jaccard similarity between text2->text6 is 0.052846371999723314\n",
      "Jaccard similarity between text2->text7 is 0.16477230674885546\n",
      "Jaccard similarity between text2->text8 is 0.02434018651208008\n",
      "Jaccard similarity between text2->text9 is 0.24845407699302818\n",
      "Jaccard similarity between text3->text4 is 0.09615537268806393\n",
      "Jaccard similarity between text3->text5 is 0.07764665640499402\n",
      "Jaccard similarity between text3->text6 is 0.08306090846524432\n",
      "Jaccard similarity between text3->text7 is 0.07710334452031015\n",
      "Jaccard similarity between text3->text8 is 0.026792635658914727\n",
      "Jaccard similarity between text3->text9 is 0.15443413729128014\n",
      "Jaccard similarity between text4->text5 is 0.07819161004501869\n",
      "Jaccard similarity between text4->text6 is 0.04058034412365121\n",
      "Jaccard similarity between text4->text7 is 0.21737638748738647\n",
      "Jaccard similarity between text4->text8 is 0.019874666989585857\n",
      "Jaccard similarity between text4->text9 is 0.1811728787686045\n",
      "Jaccard similarity between text5->text6 is 0.09666789953263172\n",
      "Jaccard similarity between text5->text7 is 0.09727339719970524\n",
      "Jaccard similarity between text5->text8 is 0.04397607934655776\n",
      "Jaccard similarity between text5->text9 is 0.14648669265977285\n",
      "Jaccard similarity between text6->text7 is 0.04594608062988987\n",
      "Jaccard similarity between text6->text8 is 0.055597867479055596\n",
      "Jaccard similarity between text6->text9 is 0.09862893906742871\n",
      "Jaccard similarity between text7->text8 is 0.0252216235007822\n",
      "Jaccard similarity between text7->text9 is 0.15856619613975884\n",
      "Jaccard similarity between text8->text9 is 0.03288297238227697\n"
     ]
    }
   ],
   "source": [
    "def fifth():\n",
    "    bag_of_words = []\n",
    "    bag_of_words.append(list_of_words_from_text(text1))      # text1\n",
    "    bag_of_words.append(list_of_words_from_text(text2))      # text2      \n",
    "    bag_of_words.append(list_of_words_from_text(text3))      # text3\n",
    "    bag_of_words.append(list_of_words_from_text(text4))      # text4\n",
    "    bag_of_words.append(list_of_words_from_text(text5))      # text5\n",
    "    bag_of_words.append(list_of_words_from_text(text6))      # text6\n",
    "    bag_of_words.append(list_of_words_from_text(text7))      # text7\n",
    "    bag_of_words.append(list_of_words_from_text(text8))      # text8\n",
    "    bag_of_words.append(list_of_words_from_text(text9))      # text9\n",
    "    sim_dic = {}\n",
    "    #print(bag_of_words[0])\n",
    "    for i in range(len(bag_of_words)):\n",
    "        for j in range(i+1,len(bag_of_words)):\n",
    "            d = jaccard_sim_bags(bag_of_words[i], bag_of_words[j])\n",
    "            #print(d)\n",
    "            sim_dic.update(dict({\"text\"+str(i+1) + \"->\" + \"text\"+str(j+1): d}))\n",
    "    return sim_dic\n",
    "temp = fifth()\n",
    "for key in temp:\n",
    "    print(\"Jaccard similarity between \" + str(key) + \" is \" + str(temp[key]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7f28d9",
   "metadata": {},
   "source": [
    "#### 6) (use NLTK) Let us consider a metric space (S; d), where S is the set of words from text1 and d is the Hamming distance. Find diameter of (S; d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "a1179686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# diameter\n",
    "def sixth(text = text1):\n",
    "    words = set(list_of_words_from_text(text))\n",
    "    dic = {}\n",
    "    maximum = 0\n",
    "    for pairs in combinations(words,2):\n",
    "        a = pairs[0]\n",
    "        b = pairs[1]\n",
    "        if((a,b) not in dic.keys() or (b,a) not in dic.keys() and a != b):\n",
    "            if(min(len(a),len(b)) > maximum):\n",
    "                dist = hamming_dist(a,b)\n",
    "                dic.update({(a,b):dist})\n",
    "                if(dist > maximum):\n",
    "                    maximum = dist\n",
    "    return maximum\n",
    "   # for a,b in pairs:\n",
    "    #    dist = hamming_dist(a,b)\n",
    "   #     dic.update({(a,b):dist})\n",
    "\n",
    "\n",
    "sixth()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8ac024",
   "metadata": {},
   "source": [
    "#### 7)  (use NLTK) Construct a dictionary that assigns each pair of consecutive words in text1 the Jaccard similarity between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "7d3b0325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('moby', 'dick'): 0.0,\n",
       " ('dick', 'by'): 0.0,\n",
       " ('by', 'herman'): 0.0,\n",
       " ('herman', 'melville'): 0.2222222222222222,\n",
       " ('melville', '1851'): 0.0,\n",
       " ('1851', 'etymology'): 0.0,\n",
       " ('etymology', 'supplied'): 0.16666666666666663,\n",
       " ('supplied', 'by'): 0.0,\n",
       " ('by', 'a'): 0.0,\n",
       " ('a', 'late'): 0.25,\n",
       " ('late', 'consumptive'): 0.15384615384615385,\n",
       " ('consumptive', 'usher'): 0.23076923076923073,\n",
       " ('usher', 'to'): 0.0,\n",
       " ('to', 'a'): 0.0,\n",
       " ('a', 'grammar'): 0.25,\n",
       " ('grammar', 'school'): 0.0,\n",
       " ('school', 'the'): 0.1428571428571429,\n",
       " ('the', 'pale'): 0.16666666666666663,\n",
       " ('pale', 'usher'): 0.125,\n",
       " ('usher', 'threadbare'): 0.33333333333333337,\n",
       " ('threadbare', 'in'): 0.0,\n",
       " ('in', 'coat'): 0.0,\n",
       " ('coat', 'heart'): 0.2857142857142857,\n",
       " ('heart', 'body'): 0.0,\n",
       " ('body', 'and'): 0.16666666666666663,\n",
       " ('and', 'brain'): 0.33333333333333337,\n",
       " ('brain', 'i'): 0.19999999999999996,\n",
       " ('i', 'see'): 0.0,\n",
       " ('see', 'him'): 0.0,\n",
       " ('him', 'now'): 0.0,\n",
       " ('now', 'he'): 0.0,\n",
       " ('he', 'was'): 0.0,\n",
       " ('was', 'ever'): 0.0,\n",
       " ('ever', 'dusting'): 0.0,\n",
       " ('dusting', 'his'): 0.25,\n",
       " ('his', 'old'): 0.0,\n",
       " ('old', 'lexicons'): 0.2222222222222222,\n",
       " ('lexicons', 'and'): 0.09999999999999998,\n",
       " ('and', 'grammars'): 0.1428571428571429,\n",
       " ('grammars', 'with'): 0.0,\n",
       " ('with', 'a'): 0.0,\n",
       " ('a', 'queer'): 0.0,\n",
       " ('queer', 'handkerchief'): 0.16666666666666663,\n",
       " ('handkerchief', 'mockingly'): 0.2666666666666667,\n",
       " ('mockingly', 'embellished'): 0.2142857142857143,\n",
       " ('embellished', 'with'): 0.19999999999999996,\n",
       " ('with', 'all'): 0.0,\n",
       " ('all', 'the'): 0.0,\n",
       " ('the', 'gay'): 0.0,\n",
       " ('gay', 'flags'): 0.33333333333333337,\n",
       " ('flags', 'of'): 0.16666666666666663,\n",
       " ('of', 'all'): 0.0,\n",
       " ('the', 'known'): 0.0,\n",
       " ('known', 'nations'): 0.25,\n",
       " ('nations', 'of'): 0.1428571428571429,\n",
       " ('of', 'the'): 0.0,\n",
       " ('the', 'world'): 0.0,\n",
       " ('world', 'he'): 0.0,\n",
       " ('he', 'loved'): 0.16666666666666663,\n",
       " ('loved', 'to'): 0.16666666666666663,\n",
       " ('to', 'dust'): 0.19999999999999996,\n",
       " ('dust', 'his'): 0.16666666666666663,\n",
       " ('old', 'grammars'): 0.0,\n",
       " ('grammars', 'it'): 0.0,\n",
       " ('it', 'somehow'): 0.0,\n",
       " ('somehow', 'mildly'): 0.09999999999999998,\n",
       " ('mildly', 'reminded'): 0.375,\n",
       " ('reminded', 'him'): 0.2857142857142857,\n",
       " ('him', 'of'): 0.0,\n",
       " ('of', 'his'): 0.0,\n",
       " ('his', 'mortality'): 0.09999999999999998,\n",
       " ('mortality', 'while'): 0.18181818181818177,\n",
       " ('while', 'you'): 0.0,\n",
       " ('you', 'take'): 0.0,\n",
       " ('take', 'in'): 0.0,\n",
       " ('in', 'hand'): 0.19999999999999996,\n",
       " ('hand', 'to'): 0.0,\n",
       " ('to', 'school'): 0.16666666666666663,\n",
       " ('school', 'others'): 0.375,\n",
       " ('others', 'and'): 0.0,\n",
       " ('and', 'to'): 0.0,\n",
       " ('to', 'teach'): 0.16666666666666663,\n",
       " ('teach', 'them'): 0.5,\n",
       " ('them', 'by'): 0.0,\n",
       " ('by', 'what'): 0.0,\n",
       " ('what', 'name'): 0.1428571428571429,\n",
       " ('name', 'a'): 0.25,\n",
       " ('a', 'whale'): 0.19999999999999996,\n",
       " ('whale', 'fish'): 0.125,\n",
       " ('fish', 'is'): 0.5,\n",
       " ('is', 'to'): 0.0,\n",
       " ('to', 'be'): 0.0,\n",
       " ('be', 'called'): 0.16666666666666663,\n",
       " ('called', 'in'): 0.0,\n",
       " ('in', 'our'): 0.0,\n",
       " ('our', 'tongue'): 0.2857142857142857,\n",
       " ('tongue', 'leaving'): 0.30000000000000004,\n",
       " ('leaving', 'out'): 0.0,\n",
       " ('out', 'through'): 0.5,\n",
       " ('through', 'ignorance'): 0.2727272727272727,\n",
       " ('ignorance', 'the'): 0.09999999999999998,\n",
       " ('the', 'letter'): 0.4,\n",
       " ('letter', 'h'): 0.0,\n",
       " ('h', 'which'): 0.25,\n",
       " ('which', 'almost'): 0.0,\n",
       " ('almost', 'alone'): 0.375,\n",
       " ('alone', 'maketh'): 0.2222222222222222,\n",
       " ('maketh', 'the'): 0.5,\n",
       " ('the', 'signification'): 0.09090909090909094,\n",
       " ('signification', 'of'): 0.2222222222222222,\n",
       " ('the', 'word'): 0.0,\n",
       " ('word', 'you'): 0.16666666666666663,\n",
       " ('you', 'deliver'): 0.0,\n",
       " ('deliver', 'that'): 0.0,\n",
       " ('that', 'which'): 0.16666666666666663,\n",
       " ('which', 'is'): 0.19999999999999996,\n",
       " ('is', 'not'): 0.0,\n",
       " ('not', 'true'): 0.16666666666666663,\n",
       " ('true', 'hackluyt'): 0.19999999999999996,\n",
       " ('hackluyt', 'whale'): 0.30000000000000004,\n",
       " ('whale', 'sw'): 0.16666666666666663,\n",
       " ('sw', 'and'): 0.0,\n",
       " ('and', 'dan'): 1.0,\n",
       " ('dan', 'hval'): 0.16666666666666663,\n",
       " ('hval', 'this'): 0.1428571428571429,\n",
       " ('this', 'animal'): 0.125,\n",
       " ('animal', 'is'): 0.16666666666666663,\n",
       " ('is', 'named'): 0.0,\n",
       " ('named', 'from'): 0.125,\n",
       " ('from', 'roundness'): 0.2222222222222222,\n",
       " ('roundness', 'or'): 0.2857142857142857,\n",
       " ('or', 'rolling'): 0.33333333333333337,\n",
       " ('rolling', 'for'): 0.2857142857142857,\n",
       " ('for', 'in'): 0.0,\n",
       " ('in', 'dan'): 0.25,\n",
       " ('dan', 'hvalt'): 0.1428571428571429,\n",
       " ('hvalt', 'is'): 0.0,\n",
       " ('is', 'arched'): 0.0,\n",
       " ('arched', 'or'): 0.1428571428571429,\n",
       " ('or', 'vaulted'): 0.0,\n",
       " ('vaulted', 'webster'): 0.18181818181818177,\n",
       " ('webster', 's'): 0.16666666666666663,\n",
       " ('s', 'dictionary'): 0.0,\n",
       " ('dictionary', 'whale'): 0.07692307692307687,\n",
       " ('whale', 'it'): 0.0,\n",
       " ('it', 'is'): 0.33333333333333337,\n",
       " ('is', 'more'): 0.0,\n",
       " ('more', 'immediately'): 0.19999999999999996,\n",
       " ('immediately', 'from'): 0.09090909090909094,\n",
       " ('from', 'the'): 0.0,\n",
       " ('the', 'dut'): 0.19999999999999996,\n",
       " ('dut', 'and'): 0.19999999999999996,\n",
       " ('and', 'ger'): 0.0,\n",
       " ('ger', 'wallen'): 0.1428571428571429,\n",
       " ('wallen', 'a'): 0.19999999999999996,\n",
       " ('a', 's'): 0.0,\n",
       " ('s', 'walw'): 0.0,\n",
       " ('walw', 'ian'): 0.19999999999999996,\n",
       " ('ian', 'to'): 0.0,\n",
       " ('to', 'roll'): 0.25,\n",
       " ('to', 'wallow'): 0.19999999999999996,\n",
       " ('wallow', 'richardson'): 0.18181818181818177,\n",
       " ('richardson', 's'): 0.11111111111111116,\n",
       " ('dictionary', 'ketos'): 0.16666666666666663,\n",
       " ('ketos', 'greek'): 0.2857142857142857,\n",
       " ('greek', 'cetus'): 0.125,\n",
       " ('cetus', 'latin'): 0.11111111111111116,\n",
       " ('latin', 'whoel'): 0.11111111111111116,\n",
       " ('whoel', 'anglo'): 0.25,\n",
       " ('anglo', 'saxon'): 0.4285714285714286,\n",
       " ('saxon', 'hvalt'): 0.11111111111111116,\n",
       " ('hvalt', 'danish'): 0.2222222222222222,\n",
       " ('danish', 'wal'): 0.125,\n",
       " ('wal', 'dutch'): 0.0,\n",
       " ('dutch', 'hwal'): 0.125,\n",
       " ('hwal', 'swedish'): 0.25,\n",
       " ('swedish', 'whale'): 0.375,\n",
       " ('whale', 'icelandic'): 0.33333333333333337,\n",
       " ('whale', 'english'): 0.33333333333333337,\n",
       " ('english', 'baleine'): 0.4444444444444444,\n",
       " ('baleine', 'french'): 0.19999999999999996,\n",
       " ('french', 'ballena'): 0.2222222222222222,\n",
       " ('ballena', 'spanish'): 0.2222222222222222,\n",
       " ('spanish', 'pekee'): 0.125,\n",
       " ('pekee', 'nuee'): 0.19999999999999996,\n",
       " ('nuee', 'fegee'): 0.19999999999999996,\n",
       " ('fegee', 'pekee'): 0.19999999999999996,\n",
       " ('nuee', 'erromangoan'): 0.25,\n",
       " ('erromangoan', 'extracts'): 0.2727272727272727,\n",
       " ('extracts', 'supplied'): 0.16666666666666663,\n",
       " ('a', 'sub'): 0.0,\n",
       " ('sub', 'librarian'): 0.125,\n",
       " ('librarian', 'it'): 0.1428571428571429,\n",
       " ('it', 'will'): 0.25,\n",
       " ('will', 'be'): 0.0,\n",
       " ('be', 'seen'): 0.25,\n",
       " ('seen', 'that'): 0.0,\n",
       " ('that', 'this'): 0.4,\n",
       " ('this', 'mere'): 0.0,\n",
       " ('mere', 'painstaking'): 0.0,\n",
       " ('painstaking', 'burrower'): 0.0,\n",
       " ('burrower', 'and'): 0.0,\n",
       " ('and', 'grub'): 0.0,\n",
       " ('grub', 'worm'): 0.1428571428571429,\n",
       " ('worm', 'of'): 0.19999999999999996,\n",
       " ('of', 'a'): 0.0,\n",
       " ('a', 'poor'): 0.0,\n",
       " ('poor', 'devil'): 0.0,\n",
       " ('devil', 'of'): 0.0,\n",
       " ('sub', 'appears'): 0.1428571428571429,\n",
       " ('appears', 'to'): 0.0,\n",
       " ('to', 'have'): 0.0,\n",
       " ('have', 'gone'): 0.1428571428571429,\n",
       " ('gone', 'through'): 0.25,\n",
       " ('through', 'the'): 0.2857142857142857,\n",
       " ('the', 'long'): 0.0,\n",
       " ('long', 'vaticans'): 0.09999999999999998,\n",
       " ('vaticans', 'and'): 0.25,\n",
       " ('and', 'street'): 0.0,\n",
       " ('street', 'stalls'): 0.33333333333333337,\n",
       " ('stalls', 'of'): 0.0,\n",
       " ('the', 'earth'): 0.6,\n",
       " ('earth', 'picking'): 0.0,\n",
       " ('picking', 'up'): 0.1428571428571429,\n",
       " ('up', 'whatever'): 0.0,\n",
       " ('whatever', 'random'): 0.18181818181818177,\n",
       " ('random', 'allusions'): 0.30000000000000004,\n",
       " ('allusions', 'to'): 0.125,\n",
       " ('to', 'whales'): 0.0,\n",
       " ('whales', 'he'): 0.33333333333333337,\n",
       " ('he', 'could'): 0.0,\n",
       " ('could', 'anyways'): 0.0,\n",
       " ('anyways', 'find'): 0.125,\n",
       " ('find', 'in'): 0.5,\n",
       " ('in', 'any'): 0.25,\n",
       " ('any', 'book'): 0.0,\n",
       " ('book', 'whatsoever'): 0.09090909090909094,\n",
       " ('whatsoever', 'sacred'): 0.36363636363636365,\n",
       " ('sacred', 'or'): 0.1428571428571429,\n",
       " ('or', 'profane'): 0.2857142857142857,\n",
       " ('profane', 'therefore'): 0.4444444444444444,\n",
       " ('therefore', 'you'): 0.125,\n",
       " ('you', 'must'): 0.16666666666666663,\n",
       " ('must', 'not'): 0.16666666666666663,\n",
       " ('not', 'in'): 0.25,\n",
       " ('in', 'every'): 0.0,\n",
       " ('every', 'case'): 0.1428571428571429,\n",
       " ('case', 'at'): 0.19999999999999996,\n",
       " ('at', 'least'): 0.4,\n",
       " ('least', 'take'): 0.5,\n",
       " ('take', 'the'): 0.4,\n",
       " ('the', 'higgledy'): 0.25,\n",
       " ('higgledy', 'piggledy'): 0.75,\n",
       " ('piggledy', 'whale'): 0.19999999999999996,\n",
       " ('whale', 'statements'): 0.2222222222222222,\n",
       " ('statements', 'however'): 0.09090909090909094,\n",
       " ('however', 'authentic'): 0.16666666666666663,\n",
       " ('authentic', 'in'): 0.25,\n",
       " ('in', 'these'): 0.0,\n",
       " ('these', 'extracts'): 0.375,\n",
       " ('extracts', 'for'): 0.11111111111111116,\n",
       " ('for', 'veritable'): 0.09999999999999998,\n",
       " ('veritable', 'gospel'): 0.16666666666666663,\n",
       " ('gospel', 'cetology'): 0.4444444444444444,\n",
       " ('cetology', 'far'): 0.0,\n",
       " ('far', 'from'): 0.4,\n",
       " ('from', 'it'): 0.0,\n",
       " ('it', 'as'): 0.0,\n",
       " ('as', 'touching'): 0.0,\n",
       " ('touching', 'the'): 0.2222222222222222,\n",
       " ('the', 'ancient'): 0.2857142857142857,\n",
       " ('ancient', 'authors'): 0.18181818181818177,\n",
       " ('authors', 'generally'): 0.16666666666666663,\n",
       " ('generally', 'as'): 0.125,\n",
       " ('as', 'well'): 0.0,\n",
       " ('as', 'the'): 0.0,\n",
       " ('the', 'poets'): 0.33333333333333337,\n",
       " ('poets', 'here'): 0.1428571428571429,\n",
       " ('here', 'appearing'): 0.25,\n",
       " ('appearing', 'these'): 0.09999999999999998,\n",
       " ('extracts', 'are'): 0.4285714285714286,\n",
       " ('are', 'solely'): 0.1428571428571429,\n",
       " ('solely', 'valuable'): 0.2222222222222222,\n",
       " ('valuable', 'or'): 0.0,\n",
       " ('or', 'entertaining'): 0.125,\n",
       " ('entertaining', 'as'): 0.125,\n",
       " ('as', 'affording'): 0.11111111111111116,\n",
       " ('affording', 'a'): 0.125,\n",
       " ('a', 'glancing'): 0.16666666666666663,\n",
       " ('glancing', 'bird'): 0.11111111111111116,\n",
       " ('bird', 's'): 0.0,\n",
       " ('s', 'eye'): 0.0,\n",
       " ('eye', 'view'): 0.19999999999999996,\n",
       " ('view', 'of'): 0.0,\n",
       " ('of', 'what'): 0.0,\n",
       " ('what', 'has'): 0.4,\n",
       " ('has', 'been'): 0.0,\n",
       " ('been', 'promiscuously'): 0.0,\n",
       " ('promiscuously', 'said'): 0.16666666666666663,\n",
       " ('said', 'thought'): 0.0,\n",
       " ('thought', 'fancied'): 0.0,\n",
       " ('fancied', 'and'): 0.4285714285714286,\n",
       " ('and', 'sung'): 0.16666666666666663,\n",
       " ('sung', 'of'): 0.0,\n",
       " ('of', 'leviathan'): 0.0,\n",
       " ('leviathan', 'by'): 0.0,\n",
       " ('by', 'many'): 0.19999999999999996,\n",
       " ('many', 'nations'): 0.25,\n",
       " ('nations', 'and'): 0.2857142857142857,\n",
       " ('and', 'generations'): 0.19999999999999996,\n",
       " ('generations', 'including'): 0.23076923076923073,\n",
       " ('including', 'our'): 0.11111111111111116,\n",
       " ('our', 'own'): 0.19999999999999996,\n",
       " ('own', 'so'): 0.25,\n",
       " ('so', 'fare'): 0.0,\n",
       " ('fare', 'thee'): 0.16666666666666663,\n",
       " ('thee', 'well'): 0.19999999999999996,\n",
       " ('well', 'poor'): 0.0,\n",
       " ('sub', 'whose'): 0.1428571428571429,\n",
       " ('whose', 'commentator'): 0.18181818181818177,\n",
       " ('commentator', 'i'): 0.0,\n",
       " ('i', 'am'): 0.0,\n",
       " ('am', 'thou'): 0.0,\n",
       " ('thou', 'belongest'): 0.19999999999999996,\n",
       " ('belongest', 'to'): 0.25,\n",
       " ('to', 'that'): 0.25,\n",
       " ('that', 'hopeless'): 0.125,\n",
       " ('hopeless', 'sallow'): 0.375,\n",
       " ('sallow', 'tribe'): 0.0,\n",
       " ('tribe', 'which'): 0.125,\n",
       " ('which', 'no'): 0.0,\n",
       " ('no', 'wine'): 0.19999999999999996,\n",
       " ('wine', 'of'): 0.0,\n",
       " ('of', 'this'): 0.0,\n",
       " ('this', 'world'): 0.0,\n",
       " ('world', 'will'): 0.33333333333333337,\n",
       " ('will', 'ever'): 0.0,\n",
       " ('ever', 'warm'): 0.16666666666666663,\n",
       " ('warm', 'and'): 0.16666666666666663,\n",
       " ('and', 'for'): 0.0,\n",
       " ('for', 'whom'): 0.16666666666666663,\n",
       " ('whom', 'even'): 0.0,\n",
       " ('even', 'pale'): 0.16666666666666663,\n",
       " ('pale', 'sherry'): 0.125,\n",
       " ('sherry', 'would'): 0.0,\n",
       " ('would', 'be'): 0.0,\n",
       " ('be', 'too'): 0.0,\n",
       " ('too', 'rosy'): 0.19999999999999996,\n",
       " ('rosy', 'strong'): 0.4285714285714286,\n",
       " ('strong', 'but'): 0.125,\n",
       " ('but', 'with'): 0.16666666666666663,\n",
       " ('with', 'whom'): 0.33333333333333337,\n",
       " ('whom', 'one'): 0.16666666666666663,\n",
       " ('one', 'sometimes'): 0.2857142857142857,\n",
       " ('sometimes', 'loves'): 0.375,\n",
       " ('loves', 'to'): 0.16666666666666663,\n",
       " ('to', 'sit'): 0.25,\n",
       " ('sit', 'and'): 0.0,\n",
       " ('and', 'feel'): 0.0,\n",
       " ('feel', 'poor'): 0.0,\n",
       " ('poor', 'devilish'): 0.0,\n",
       " ('devilish', 'too'): 0.0,\n",
       " ('too', 'and'): 0.0,\n",
       " ('and', 'grow'): 0.0,\n",
       " ('grow', 'convivial'): 0.09999999999999998,\n",
       " ('convivial', 'upon'): 0.2222222222222222,\n",
       " ('upon', 'tears'): 0.0,\n",
       " ('tears', 'and'): 0.1428571428571429,\n",
       " ('and', 'say'): 0.19999999999999996,\n",
       " ('say', 'to'): 0.0,\n",
       " ('to', 'them'): 0.19999999999999996,\n",
       " ('them', 'bluntly'): 0.11111111111111116,\n",
       " ('bluntly', 'with'): 0.11111111111111116,\n",
       " ('with', 'full'): 0.0,\n",
       " ('full', 'eyes'): 0.0,\n",
       " ('eyes', 'and'): 0.0,\n",
       " ('and', 'empty'): 0.0,\n",
       " ('empty', 'glasses'): 0.11111111111111116,\n",
       " ('glasses', 'and'): 0.1428571428571429,\n",
       " ('and', 'in'): 0.25,\n",
       " ('not', 'altogether'): 0.2222222222222222,\n",
       " ('altogether', 'unpleasant'): 0.33333333333333337,\n",
       " ('unpleasant', 'sadness'): 0.4444444444444444,\n",
       " ('sadness', 'give'): 0.125,\n",
       " ('give', 'it'): 0.19999999999999996,\n",
       " ('it', 'up'): 0.0,\n",
       " ('up', 'sub'): 0.25,\n",
       " ('sub', 'subs'): 1.0,\n",
       " ('subs', 'for'): 0.0,\n",
       " ('for', 'by'): 0.0,\n",
       " ('by', 'how'): 0.0,\n",
       " ('how', 'much'): 0.16666666666666663,\n",
       " ('much', 'the'): 0.16666666666666663,\n",
       " ('the', 'more'): 0.16666666666666663,\n",
       " ('more', 'pains'): 0.0,\n",
       " ('pains', 'ye'): 0.0,\n",
       " ('ye', 'take'): 0.19999999999999996,\n",
       " ('take', 'to'): 0.19999999999999996,\n",
       " ('to', 'please'): 0.0,\n",
       " ('please', 'the'): 0.1428571428571429,\n",
       " ('world', 'by'): 0.0,\n",
       " ('by', 'so'): 0.0,\n",
       " ('so', 'much'): 0.0,\n",
       " ('more', 'shall'): 0.0,\n",
       " ('shall', 'ye'): 0.0,\n",
       " ('ye', 'for'): 0.0,\n",
       " ('for', 'ever'): 0.19999999999999996,\n",
       " ('ever', 'go'): 0.0,\n",
       " ('go', 'thankless'): 0.0,\n",
       " ('thankless', 'would'): 0.08333333333333337,\n",
       " ('would', 'that'): 0.0,\n",
       " ('that', 'i'): 0.0,\n",
       " ('i', 'could'): 0.0,\n",
       " ('could', 'clear'): 0.25,\n",
       " ('clear', 'out'): 0.0,\n",
       " ('out', 'hampton'): 0.25,\n",
       " ('hampton', 'court'): 0.19999999999999996,\n",
       " ('court', 'and'): 0.0,\n",
       " ('and', 'the'): 0.0,\n",
       " ('the', 'tuileries'): 0.25,\n",
       " ('tuileries', 'for'): 0.11111111111111116,\n",
       " ('ye', 'but'): 0.0,\n",
       " ('but', 'gulp'): 0.16666666666666663,\n",
       " ('gulp', 'down'): 0.0,\n",
       " ('down', 'your'): 0.1428571428571429,\n",
       " ('your', 'tears'): 0.125,\n",
       " ('and', 'hie'): 0.0,\n",
       " ('hie', 'aloft'): 0.0,\n",
       " ('aloft', 'to'): 0.4,\n",
       " ('to', 'the'): 0.25,\n",
       " ('the', 'royal'): 0.0,\n",
       " ('royal', 'mast'): 0.125,\n",
       " ('mast', 'with'): 0.1428571428571429,\n",
       " ('with', 'your'): 0.0,\n",
       " ('your', 'hearts'): 0.11111111111111116,\n",
       " ('hearts', 'for'): 0.125,\n",
       " ('for', 'your'): 0.4,\n",
       " ('your', 'friends'): 0.09999999999999998,\n",
       " ('friends', 'who'): 0.0,\n",
       " ('who', 'have'): 0.16666666666666663,\n",
       " ('gone', 'before'): 0.2857142857142857,\n",
       " ('before', 'are'): 0.33333333333333337,\n",
       " ('are', 'clearing'): 0.375,\n",
       " ('clearing', 'out'): 0.0,\n",
       " ('out', 'the'): 0.19999999999999996,\n",
       " ('the', 'seven'): 0.16666666666666663,\n",
       " ('seven', 'storied'): 0.2222222222222222,\n",
       " ('storied', 'heavens'): 0.18181818181818177,\n",
       " ('heavens', 'and'): 0.2857142857142857,\n",
       " ('and', 'making'): 0.2857142857142857,\n",
       " ('making', 'refugees'): 0.09090909090909094,\n",
       " ('refugees', 'of'): 0.1428571428571429,\n",
       " ('of', 'long'): 0.19999999999999996,\n",
       " ('long', 'pampered'): 0.0,\n",
       " ('pampered', 'gabriel'): 0.30000000000000004,\n",
       " ('gabriel', 'michael'): 0.4,\n",
       " ('michael', 'and'): 0.11111111111111116,\n",
       " ('and', 'raphael'): 0.125,\n",
       " ('raphael', 'against'): 0.09090909090909094,\n",
       " ('against', 'your'): 0.0,\n",
       " ('your', 'coming'): 0.11111111111111116,\n",
       " ('coming', 'here'): 0.0,\n",
       " ('here', 'ye'): 0.25,\n",
       " ('ye', 'strike'): 0.1428571428571429,\n",
       " ('strike', 'but'): 0.125,\n",
       " ('but', 'splintered'): 0.09090909090909094,\n",
       " ('splintered', 'hearts'): 0.36363636363636365,\n",
       " ('hearts', 'together'): 0.5,\n",
       " ('together', 'there'): 0.6666666666666667,\n",
       " ('there', 'ye'): 0.19999999999999996,\n",
       " ('shall', 'strike'): 0.11111111111111116,\n",
       " ('strike', 'unsplinterable'): 0.41666666666666663,\n",
       " ('unsplinterable', 'glasses'): 0.33333333333333337,\n",
       " ('glasses', 'extracts'): 0.33333333333333337,\n",
       " ('extracts', 'and'): 0.11111111111111116,\n",
       " ('and', 'god'): 0.19999999999999996,\n",
       " ('god', 'created'): 0.125,\n",
       " ('created', 'great'): 0.5714285714285714,\n",
       " ('great', 'whales'): 0.2222222222222222,\n",
       " ('whales', 'genesis'): 0.2222222222222222,\n",
       " ('genesis', 'leviathan'): 0.30000000000000004,\n",
       " ('leviathan', 'maketh'): 0.4,\n",
       " ('maketh', 'a'): 0.16666666666666663,\n",
       " ('a', 'path'): 0.25,\n",
       " ('path', 'to'): 0.19999999999999996,\n",
       " ('to', 'shine'): 0.0,\n",
       " ('shine', 'after'): 0.11111111111111116,\n",
       " ('after', 'him'): 0.0,\n",
       " ('him', 'one'): 0.0,\n",
       " ('one', 'would'): 0.1428571428571429,\n",
       " ('would', 'think'): 0.0,\n",
       " ('think', 'the'): 0.33333333333333337,\n",
       " ('the', 'deep'): 0.19999999999999996,\n",
       " ('deep', 'to'): 0.0,\n",
       " ('be', 'hoary'): 0.0,\n",
       " ('hoary', 'job'): 0.1428571428571429,\n",
       " ('job', 'now'): 0.19999999999999996,\n",
       " ('now', 'the'): 0.0,\n",
       " ('the', 'lord'): 0.0,\n",
       " ('lord', 'had'): 0.16666666666666663,\n",
       " ('had', 'prepared'): 0.33333333333333337,\n",
       " ('prepared', 'a'): 0.19999999999999996,\n",
       " ('a', 'great'): 0.19999999999999996,\n",
       " ('great', 'fish'): 0.0,\n",
       " ('fish', 'to'): 0.0,\n",
       " ('to', 'swallow'): 0.16666666666666663,\n",
       " ('swallow', 'up'): 0.0,\n",
       " ('up', 'jonah'): 0.0,\n",
       " ('jonah', 'there'): 0.125,\n",
       " ('there', 'go'): 0.0,\n",
       " ('go', 'the'): 0.0,\n",
       " ('the', 'ships'): 0.16666666666666663,\n",
       " ('ships', 'there'): 0.1428571428571429,\n",
       " ('there', 'is'): 0.0,\n",
       " ('is', 'that'): 0.0,\n",
       " ('that', 'leviathan'): 0.375,\n",
       " ('leviathan', 'whom'): 0.09090909090909094,\n",
       " ('whom', 'thou'): 0.33333333333333337,\n",
       " ('thou', 'hast'): 0.33333333333333337,\n",
       " ('hast', 'made'): 0.1428571428571429,\n",
       " ('made', 'to'): 0.0,\n",
       " ('to', 'play'): 0.0,\n",
       " ('play', 'therein'): 0.0,\n",
       " ('therein', 'psalms'): 0.0,\n",
       " ('psalms', 'in'): 0.0,\n",
       " ('in', 'that'): 0.0,\n",
       " ('that', 'day'): 0.19999999999999996,\n",
       " ('day', 'the'): 0.0,\n",
       " ('lord', 'with'): 0.0,\n",
       " ('with', 'his'): 0.4,\n",
       " ('his', 'sore'): 0.16666666666666663,\n",
       " ('sore', 'and'): 0.0,\n",
       " ('and', 'great'): 0.1428571428571429,\n",
       " ('and', 'strong'): 0.125,\n",
       " ('strong', 'sword'): 0.375,\n",
       " ('sword', 'shall'): 0.125,\n",
       " ('shall', 'punish'): 0.25,\n",
       " ('punish', 'leviathan'): 0.2727272727272727,\n",
       " ('leviathan', 'the'): 0.375,\n",
       " ('the', 'piercing'): 0.11111111111111116,\n",
       " ('piercing', 'serpent'): 0.4444444444444444,\n",
       " ('serpent', 'even'): 0.2857142857142857,\n",
       " ('even', 'leviathan'): 0.375,\n",
       " ('that', 'crooked'): 0.0,\n",
       " ('crooked', 'serpent'): 0.19999999999999996,\n",
       " ('serpent', 'and'): 0.125,\n",
       " ('and', 'he'): 0.0,\n",
       " ('he', 'shall'): 0.19999999999999996,\n",
       " ('shall', 'slay'): 0.6,\n",
       " ('slay', 'the'): 0.0,\n",
       " ('the', 'dragon'): 0.0,\n",
       " ('dragon', 'that'): 0.125,\n",
       " ('is', 'in'): 0.33333333333333337,\n",
       " ('in', 'the'): 0.0,\n",
       " ('the', 'sea'): 0.19999999999999996,\n",
       " ('sea', 'isaiah'): 0.4,\n",
       " ('isaiah', 'and'): 0.16666666666666663,\n",
       " ('and', 'what'): 0.16666666666666663,\n",
       " ('what', 'thing'): 0.2857142857142857,\n",
       " ('thing', 'soever'): 0.0,\n",
       " ('soever', 'besides'): 0.25,\n",
       " ('besides', 'cometh'): 0.09999999999999998,\n",
       " ('cometh', 'within'): 0.2222222222222222,\n",
       " ('within', 'the'): 0.33333333333333337,\n",
       " ('the', 'chaos'): 0.1428571428571429,\n",
       " ('chaos', 'of'): 0.16666666666666663,\n",
       " ('this', 'monster'): 0.2222222222222222,\n",
       " ('monster', 's'): 0.1428571428571429,\n",
       " ('s', 'mouth'): 0.0,\n",
       " ('mouth', 'be'): 0.0,\n",
       " ('be', 'it'): 0.0,\n",
       " ('it', 'beast'): 0.16666666666666663,\n",
       " ('beast', 'boat'): 0.5,\n",
       " ('boat', 'or'): 0.19999999999999996,\n",
       " ('or', 'stone'): 0.16666666666666663,\n",
       " ('stone', 'down'): 0.2857142857142857,\n",
       " ('down', 'it'): 0.0,\n",
       " ('it', 'goes'): 0.0,\n",
       " ('goes', 'all'): 0.0,\n",
       " ('all', 'incontinently'): 0.11111111111111116,\n",
       " ('incontinently', 'that'): 0.09999999999999998,\n",
       " ('that', 'foul'): 0.0,\n",
       " ('foul', 'great'): 0.0,\n",
       " ('great', 'swallow'): 0.11111111111111116,\n",
       " ('swallow', 'of'): 0.16666666666666663,\n",
       " ('his', 'and'): 0.0,\n",
       " ('and', 'perisheth'): 0.0,\n",
       " ('perisheth', 'in'): 0.125,\n",
       " ('the', 'bottomless'): 0.25,\n",
       " ('bottomless', 'gulf'): 0.09999999999999998,\n",
       " ('gulf', 'of'): 0.19999999999999996,\n",
       " ('his', 'paunch'): 0.125,\n",
       " ('paunch', 'holland'): 0.33333333333333337,\n",
       " ('holland', 's'): 0.0,\n",
       " ('s', 'plutarch'): 0.0,\n",
       " ('s', 'morals'): 0.16666666666666663,\n",
       " ('morals', 'the'): 0.0,\n",
       " ('the', 'indian'): 0.0,\n",
       " ('indian', 'sea'): 0.16666666666666663,\n",
       " ('sea', 'breedeth'): 0.125,\n",
       " ('breedeth', 'the'): 0.5,\n",
       " ('the', 'most'): 0.16666666666666663,\n",
       " ('most', 'and'): 0.0,\n",
       " ('the', 'biggest'): 0.2857142857142857,\n",
       " ('biggest', 'fishes'): 0.375,\n",
       " ('fishes', 'that'): 0.1428571428571429,\n",
       " ('that', 'are'): 0.19999999999999996,\n",
       " ('are', 'among'): 0.1428571428571429,\n",
       " ('among', 'which'): 0.0,\n",
       " ('which', 'the'): 0.16666666666666663,\n",
       " ('the', 'whales'): 0.2857142857142857,\n",
       " ('whales', 'and'): 0.125,\n",
       " ('and', 'whirlpooles'): 0.0,\n",
       " ('whirlpooles', 'called'): 0.16666666666666663,\n",
       " ('called', 'balaene'): 0.4285714285714286,\n",
       " ('balaene', 'take'): 0.2857142857142857,\n",
       " ('take', 'up'): 0.0,\n",
       " ('up', 'as'): 0.0,\n",
       " ('as', 'much'): 0.0,\n",
       " ('much', 'in'): 0.0,\n",
       " ('in', 'length'): 0.1428571428571429,\n",
       " ('length', 'as'): 0.0,\n",
       " ('as', 'four'): 0.0,\n",
       " ('four', 'acres'): 0.125,\n",
       " ('acres', 'or'): 0.16666666666666663,\n",
       " ('or', 'arpens'): 0.1428571428571429,\n",
       " ('arpens', 'of'): 0.0,\n",
       " ('of', 'land'): 0.0,\n",
       " ('land', 'holland'): 0.6666666666666667,\n",
       " ('s', 'pliny'): 0.0,\n",
       " ('pliny', 'scarcely'): 0.19999999999999996,\n",
       " ('scarcely', 'had'): 0.11111111111111116,\n",
       " ('had', 'we'): 0.0,\n",
       " ('we', 'proceeded'): 0.1428571428571429,\n",
       " ('proceeded', 'two'): 0.125,\n",
       " ('two', 'days'): 0.0,\n",
       " ('days', 'on'): 0.0,\n",
       " ('on', 'the'): 0.0,\n",
       " ('sea', 'when'): 0.16666666666666663,\n",
       " ('when', 'about'): 0.0,\n",
       " ('about', 'sunrise'): 0.09999999999999998,\n",
       " ('sunrise', 'a'): 0.0,\n",
       " ('great', 'many'): 0.125,\n",
       " ('many', 'whales'): 0.11111111111111116,\n",
       " ('and', 'other'): 0.0,\n",
       " ('other', 'monsters'): 0.5,\n",
       " ('monsters', 'of'): 0.125,\n",
       " ('sea', 'appeared'): 0.33333333333333337,\n",
       " ('appeared', 'among'): 0.11111111111111116,\n",
       " ('among', 'the'): 0.0,\n",
       " ('the', 'former'): 0.1428571428571429,\n",
       " ('former', 'one'): 0.33333333333333337,\n",
       " ('one', 'was'): 0.0,\n",
       " ('was', 'of'): 0.0,\n",
       " ('a', 'most'): 0.0,\n",
       " ('most', 'monstrous'): 0.5714285714285714,\n",
       " ('monstrous', 'size'): 0.09999999999999998,\n",
       " ('size', 'this'): 0.33333333333333337,\n",
       " ('this', 'came'): 0.0,\n",
       " ('came', 'towards'): 0.09999999999999998,\n",
       " ('towards', 'us'): 0.125,\n",
       " ('us', 'open'): 0.0,\n",
       " ('open', 'mouthed'): 0.2222222222222222,\n",
       " ('mouthed', 'raising'): 0.0,\n",
       " ('raising', 'the'): 0.0,\n",
       " ('the', 'waves'): 0.1428571428571429,\n",
       " ('waves', 'on'): 0.0,\n",
       " ('on', 'all'): 0.0,\n",
       " ('all', 'sides'): 0.0,\n",
       " ('sides', 'and'): 0.16666666666666663,\n",
       " ('and', 'beating'): 0.25,\n",
       " ('beating', 'the'): 0.25,\n",
       " ('sea', 'before'): 0.1428571428571429,\n",
       " ('before', 'him'): 0.0,\n",
       " ('him', 'into'): 0.16666666666666663,\n",
       " ('into', 'a'): 0.0,\n",
       " ('a', 'foam'): 0.25,\n",
       " ('foam', 'tooke'): 0.1428571428571429,\n",
       " ('tooke', 's'): 0.0,\n",
       " ('s', 'lucian'): 0.0,\n",
       " ('lucian', 'the'): 0.0,\n",
       " ('the', 'true'): 0.4,\n",
       " ('true', 'history'): 0.2222222222222222,\n",
       " ('history', 'he'): 0.125,\n",
       " ('he', 'visited'): 0.1428571428571429,\n",
       " ('visited', 'this'): 0.4285714285714286,\n",
       " ('this', 'country'): 0.09999999999999998,\n",
       " ('country', 'also'): 0.09999999999999998,\n",
       " ('also', 'with'): 0.0,\n",
       " ('a', 'view'): 0.0,\n",
       " ('of', 'catching'): 0.0,\n",
       " ('catching', 'horse'): 0.09090909090909094,\n",
       " ('horse', 'whales'): 0.375,\n",
       " ('whales', 'which'): 0.25,\n",
       " ('which', 'had'): 0.16666666666666663,\n",
       " ('had', 'bones'): 0.0,\n",
       " ('bones', 'of'): 0.16666666666666663,\n",
       " ('of', 'very'): 0.0,\n",
       " ('very', 'great'): 0.2857142857142857,\n",
       " ('great', 'value'): 0.25,\n",
       " ('value', 'for'): 0.0,\n",
       " ('for', 'their'): 0.1428571428571429,\n",
       " ('their', 'teeth'): 0.6,\n",
       " ('teeth', 'of'): 0.0,\n",
       " ('of', 'which'): 0.0,\n",
       " ('which', 'he'): 0.19999999999999996,\n",
       " ('he', 'brought'): 0.125,\n",
       " ('brought', 'some'): 0.09999999999999998,\n",
       " ('some', 'to'): 0.19999999999999996,\n",
       " ('the', 'king'): 0.0,\n",
       " ('the', 'best'): 0.4,\n",
       " ('best', 'whales'): 0.25,\n",
       " ('whales', 'were'): 0.2857142857142857,\n",
       " ('were', 'catched'): 0.125,\n",
       " ('catched', 'in'): 0.0,\n",
       " ('in', 'his'): 0.25,\n",
       " ('his', 'own'): 0.0,\n",
       " ('own', 'country'): 0.25,\n",
       " ('country', 'of'): 0.125,\n",
       " ('which', 'some'): 0.0,\n",
       " ('some', 'were'): 0.16666666666666663,\n",
       " ('were', 'forty'): 0.1428571428571429,\n",
       " ('forty', 'eight'): 0.11111111111111116,\n",
       " ('eight', 'some'): 0.125,\n",
       " ('some', 'fifty'): 0.0,\n",
       " ('fifty', 'yards'): 0.125,\n",
       " ('yards', 'long'): 0.0,\n",
       " ('long', 'he'): 0.0,\n",
       " ('he', 'said'): 0.0,\n",
       " ('said', 'that'): 0.16666666666666663,\n",
       " ('that', 'he'): 0.25,\n",
       " ('one', 'of'): 0.25,\n",
       " ('of', 'six'): 0.0,\n",
       " ('six', 'who'): 0.0,\n",
       " ('who', 'had'): 0.19999999999999996,\n",
       " ('had', 'killed'): 0.1428571428571429,\n",
       " ('killed', 'sixty'): 0.11111111111111116,\n",
       " ('sixty', 'in'): 0.16666666666666663,\n",
       " ('in', 'two'): 0.0,\n",
       " ('days', 'other'): 0.0,\n",
       " ('other', 'or'): 0.4,\n",
       " ('or', 'octher'): 0.33333333333333337,\n",
       " ('octher', 's'): 0.0,\n",
       " ('s', 'verbal'): 0.0,\n",
       " ('verbal', 'narrative'): 0.4444444444444444,\n",
       " ('narrative', 'taken'): 0.5,\n",
       " ('taken', 'down'): 0.125,\n",
       " ('down', 'from'): 0.1428571428571429,\n",
       " ('from', 'his'): 0.0,\n",
       " ('his', 'mouth'): 0.1428571428571429,\n",
       " ('mouth', 'by'): 0.0,\n",
       " ('by', 'king'): 0.0,\n",
       " ('king', 'alfred'): 0.0,\n",
       " ('alfred', 'a'): 0.16666666666666663,\n",
       " ('a', 'd'): 0.0,\n",
       " ('d', '890'): 0.0,\n",
       " ('890', 'and'): 0.0,\n",
       " ('and', 'whereas'): 0.125,\n",
       " ('whereas', 'all'): 0.1428571428571429,\n",
       " ('the', 'other'): 0.6,\n",
       " ('other', 'things'): 0.2222222222222222,\n",
       " ('things', 'whether'): 0.2222222222222222,\n",
       " ('whether', 'beast'): 0.25,\n",
       " ('beast', 'or'): 0.0,\n",
       " ('or', 'vessel'): 0.0,\n",
       " ('vessel', 'that'): 0.0,\n",
       " ('that', 'enter'): 0.16666666666666663,\n",
       " ('enter', 'into'): 0.33333333333333337,\n",
       " ('into', 'the'): 0.16666666666666663,\n",
       " ('the', 'dreadful'): 0.11111111111111116,\n",
       " ('dreadful', 'gulf'): 0.375,\n",
       " ('s', 'whale'): 0.0,\n",
       " ('mouth', 'are'): 0.0,\n",
       " ('are', 'immediately'): 0.2222222222222222,\n",
       " ('immediately', 'lost'): 0.19999999999999996,\n",
       " ('lost', 'and'): 0.0,\n",
       " ('and', 'swallowed'): 0.25,\n",
       " ('swallowed', 'up'): 0.0,\n",
       " ('up', 'the'): 0.0,\n",
       " ('sea', 'gudgeon'): 0.125,\n",
       " ('gudgeon', 'retires'): 0.09999999999999998,\n",
       " ('retires', 'into'): 0.2857142857142857,\n",
       " ('into', 'it'): 0.5,\n",
       " ('it', 'in'): 0.33333333333333337,\n",
       " ('in', 'great'): 0.0,\n",
       " ('great', 'security'): 0.30000000000000004,\n",
       " ('security', 'and'): 0.0,\n",
       " ('and', 'there'): 0.0,\n",
       " ('there', 'sleeps'): 0.1428571428571429,\n",
       " ('sleeps', 'montaigne'): 0.09090909090909094,\n",
       " ('montaigne', 'apology'): 0.2727272727272727,\n",
       " ('apology', 'for'): 0.125,\n",
       " ('for', 'raimond'): 0.25,\n",
       " ('raimond', 'sebond'): 0.30000000000000004,\n",
       " ('sebond', 'let'): 0.125,\n",
       " ('let', 'us'): 0.0,\n",
       " ('us', 'fly'): 0.0,\n",
       " ('fly', 'let'): 0.19999999999999996,\n",
       " ('fly', 'old'): 0.19999999999999996,\n",
       " ('old', 'nick'): 0.0,\n",
       " ('nick', 'take'): 0.1428571428571429,\n",
       " ('take', 'me'): 0.19999999999999996,\n",
       " ('me', 'if'): 0.0,\n",
       " ('if', 'is'): 0.33333333333333337,\n",
       " ('not', 'leviathan'): 0.2222222222222222,\n",
       " ('leviathan', 'described'): 0.15384615384615385,\n",
       " ('described', 'by'): 0.125,\n",
       " ('by', 'the'): 0.0,\n",
       " ('the', 'noble'): 0.1428571428571429,\n",
       " ('noble', 'prophet'): 0.2222222222222222,\n",
       " ('prophet', 'moses'): 0.25,\n",
       " ('moses', 'in'): 0.0,\n",
       " ('the', 'life'): 0.16666666666666663,\n",
       " ('life', 'of'): 0.19999999999999996,\n",
       " ('of', 'patient'): 0.0,\n",
       " ('patient', 'job'): 0.0,\n",
       " ('job', 'rabelais'): 0.11111111111111116,\n",
       " ('rabelais', 'this'): 0.2222222222222222,\n",
       " ('this', 'whale'): 0.125,\n",
       " ('s', 'liver'): 0.0,\n",
       " ('liver', 'was'): 0.0,\n",
       " ('was', 'two'): 0.19999999999999996,\n",
       " ('two', 'cartloads'): 0.2222222222222222,\n",
       " ('cartloads', 'stowe'): 0.30000000000000004,\n",
       " ('stowe', 's'): 0.19999999999999996,\n",
       " ('s', 'annals'): 0.25,\n",
       " ('annals', 'the'): 0.0,\n",
       " ('the', 'great'): 0.33333333333333337,\n",
       " ('great', 'leviathan'): 0.30000000000000004,\n",
       " ('that', 'maketh'): 0.5,\n",
       " ('the', 'seas'): 0.19999999999999996,\n",
       " ('seas', 'to'): 0.0,\n",
       " ('to', 'seethe'): 0.19999999999999996,\n",
       " ('seethe', 'like'): 0.1428571428571429,\n",
       " ('like', 'boiling'): 0.25,\n",
       " ('boiling', 'pan'): 0.125,\n",
       " ('pan', 'lord'): 0.0,\n",
       " ('lord', 'bacon'): 0.125,\n",
       " ('bacon', 's'): 0.0,\n",
       " ('s', 'version'): 0.1428571428571429,\n",
       " ('version', 'of'): 0.125,\n",
       " ('the', 'psalms'): 0.0,\n",
       " ('psalms', 'touching'): 0.0,\n",
       " ('touching', 'that'): 0.2222222222222222,\n",
       " ('that', 'monstrous'): 0.11111111111111116,\n",
       " ('monstrous', 'bulk'): 0.09999999999999998,\n",
       " ('bulk', 'of'): 0.0,\n",
       " ('the', 'whale'): 0.33333333333333337,\n",
       " ('whale', 'or'): 0.0,\n",
       " ('or', 'ork'): 0.6666666666666667,\n",
       " ('ork', 'we'): 0.0,\n",
       " ('we', 'have'): 0.19999999999999996,\n",
       " ('have', 'received'): 0.25,\n",
       " ('received', 'nothing'): 0.09090909090909094,\n",
       " ('nothing', 'certain'): 0.30000000000000004,\n",
       " ('certain', 'they'): 0.2222222222222222,\n",
       " ('they', 'grow'): 0.0,\n",
       " ('grow', 'exceeding'): 0.09999999999999998,\n",
       " ('exceeding', 'fat'): 0.0,\n",
       " ('fat', 'insomuch'): 0.0,\n",
       " ('insomuch', 'that'): 0.09999999999999998,\n",
       " ('that', 'an'): 0.25,\n",
       " ('an', 'incredible'): 0.11111111111111116,\n",
       " ('incredible', 'quantity'): 0.15384615384615385,\n",
       " ('quantity', 'of'): 0.0,\n",
       " ('of', 'oil'): 0.25,\n",
       " ('oil', 'will'): 0.5,\n",
       " ('be', 'extracted'): 0.125,\n",
       " ('extracted', 'out'): 0.11111111111111116,\n",
       " ('out', 'of'): 0.25,\n",
       " ('one', 'whale'): 0.1428571428571429,\n",
       " ('whale', 'ibid'): 0.0,\n",
       " ('ibid', 'history'): 0.11111111111111116,\n",
       " ('history', 'of'): 0.125,\n",
       " ('life', 'and'): 0.0,\n",
       " ('and', 'death'): 0.33333333333333337,\n",
       " ('death', 'the'): 0.6,\n",
       " ('the', 'sovereignest'): 0.19999999999999996,\n",
       " ('sovereignest', 'thing'): 0.4,\n",
       " ('thing', 'on'): 0.16666666666666663,\n",
       " ('on', 'earth'): 0.0,\n",
       " ('earth', 'is'): 0.0,\n",
       " ('is', 'parmacetti'): 0.11111111111111116,\n",
       " ('parmacetti', 'for'): 0.09999999999999998,\n",
       " ('for', 'an'): 0.0,\n",
       " ('an', 'inward'): 0.33333333333333337,\n",
       " ('inward', 'bruise'): 0.19999999999999996,\n",
       " ('bruise', 'king'): 0.11111111111111116,\n",
       " ('king', 'henry'): 0.125,\n",
       " ('henry', 'very'): 0.5,\n",
       " ('very', 'like'): 0.1428571428571429,\n",
       " ('like', 'a'): 0.0,\n",
       " ('whale', 'hamlet'): 0.5714285714285714,\n",
       " ('hamlet', 'which'): 0.11111111111111116,\n",
       " ('which', 'to'): 0.0,\n",
       " ('to', 'secure'): 0.0,\n",
       " ('secure', 'no'): 0.0,\n",
       " ('no', 'skill'): 0.0,\n",
       " ('skill', 'of'): 0.0,\n",
       " ('of', 'leach'): 0.0,\n",
       " ('leach', 's'): 0.0,\n",
       " ('s', 'art'): 0.0,\n",
       " ('art', 'mote'): 0.16666666666666663,\n",
       " ('mote', 'him'): 0.16666666666666663,\n",
       " ('him', 'availle'): 0.1428571428571429,\n",
       " ('availle', 'but'): 0.0,\n",
       " ('but', 'to'): 0.25,\n",
       " ('to', 'returne'): 0.16666666666666663,\n",
       " ('returne', 'againe'): 0.25,\n",
       " ('againe', 'to'): 0.0,\n",
       " ('to', 'his'): 0.0,\n",
       " ('his', 'wound'): 0.0,\n",
       " ('wound', 's'): 0.0,\n",
       " ('s', 'worker'): 0.0,\n",
       " ('worker', 'that'): 0.0,\n",
       " ('that', 'with'): 0.4,\n",
       " ('with', 'lowly'): 0.1428571428571429,\n",
       " ('lowly', 'dart'): 0.0,\n",
       " ('dart', 'dinting'): 0.2857142857142857,\n",
       " ('dinting', 'his'): 0.1428571428571429,\n",
       " ('his', 'breast'): 0.125,\n",
       " ('breast', 'had'): 0.125,\n",
       " ('had', 'bred'): 0.16666666666666663,\n",
       " ('bred', 'his'): 0.0,\n",
       " ('his', 'restless'): 0.1428571428571429,\n",
       " ('restless', 'paine'): 0.11111111111111116,\n",
       " ('paine', 'like'): 0.2857142857142857,\n",
       " ('like', 'as'): 0.0,\n",
       " ('the', 'wounded'): 0.125,\n",
       " ('wounded', 'whale'): 0.2222222222222222,\n",
       " ('whale', 'to'): 0.0,\n",
       " ('to', 'shore'): 0.16666666666666663,\n",
       " ('shore', 'flies'): 0.25,\n",
       " ('flies', 'thro'): 0.0,\n",
       " ('thro', 'the'): 0.4,\n",
       " ('the', 'maine'): 0.1428571428571429,\n",
       " ('the', 'faerie'): 0.1428571428571429,\n",
       " ('faerie', 'queen'): 0.125,\n",
       " ('queen', 'immense'): 0.2857142857142857,\n",
       " ('immense', 'as'): 0.16666666666666663,\n",
       " ('as', 'whales'): 0.33333333333333337,\n",
       " ('the', 'motion'): 0.1428571428571429,\n",
       " ('motion', 'of'): 0.16666666666666663,\n",
       " ('of', 'whose'): 0.16666666666666663,\n",
       " ('whose', 'vast'): 0.125,\n",
       " ('vast', 'bodies'): 0.11111111111111116,\n",
       " ('bodies', 'can'): 0.0,\n",
       " ('can', 'in'): 0.25,\n",
       " ('in', 'a'): 0.0,\n",
       " ('a', 'peaceful'): 0.1428571428571429,\n",
       " ('peaceful', 'calm'): 0.375,\n",
       " ('calm', 'trouble'): 0.09999999999999998,\n",
       " ('trouble', 'the'): 0.25,\n",
       " ('the', 'ocean'): 0.1428571428571429,\n",
       " ('ocean', 'til'): 0.0,\n",
       " ('til', 'it'): 0.6666666666666667,\n",
       " ('it', 'boil'): 0.19999999999999996,\n",
       " ('boil', 'sir'): 0.16666666666666663,\n",
       " ('sir', 'william'): 0.1428571428571429,\n",
       " ('william', 'davenant'): 0.09999999999999998,\n",
       " ('davenant', 'preface'): 0.19999999999999996,\n",
       " ('preface', 'to'): 0.0,\n",
       " ('to', 'gondibert'): 0.2222222222222222,\n",
       " ('gondibert', 'what'): 0.08333333333333337,\n",
       " ('what', 'spermacetti'): 0.18181818181818177,\n",
       " ('spermacetti', 'is'): 0.2222222222222222,\n",
       " ('is', 'men'): 0.0,\n",
       " ('men', 'might'): 0.1428571428571429,\n",
       " ('might', 'justly'): 0.09999999999999998,\n",
       " ('justly', 'doubt'): 0.2222222222222222,\n",
       " ('doubt', 'since'): 0.0,\n",
       " ('since', 'the'): 0.1428571428571429,\n",
       " ('the', 'learned'): 0.125,\n",
       " ('learned', 'hosmannus'): 0.18181818181818177,\n",
       " ('hosmannus', 'in'): 0.125,\n",
       " ('his', 'work'): 0.0,\n",
       " ('work', 'of'): 0.19999999999999996,\n",
       " ('of', 'thirty'): 0.0,\n",
       " ('thirty', 'years'): 0.25,\n",
       " ('years', 'saith'): 0.25,\n",
       " ('saith', 'plainly'): 0.2222222222222222,\n",
       " ('plainly', 'nescio'): 0.19999999999999996,\n",
       " ('nescio', 'quid'): 0.11111111111111116,\n",
       " ('quid', 'sit'): 0.16666666666666663,\n",
       " ('sit', 'sir'): 0.5,\n",
       " ('sir', 't'): 0.0,\n",
       " ('t', 'browne'): 0.0,\n",
       " ('browne', 'of'): 0.1428571428571429,\n",
       " ('of', 'sperma'): 0.0,\n",
       " ('sperma', 'ceti'): 0.11111111111111116,\n",
       " ('ceti', 'and'): 0.0,\n",
       " ('the', 'sperma'): 0.125,\n",
       " ('ceti', 'whale'): 0.125,\n",
       " ('whale', 'vide'): 0.125,\n",
       " ('vide', 'his'): 0.16666666666666663,\n",
       " ('his', 'v'): 0.0,\n",
       " ('v', 'e'): 0.0,\n",
       " ('e', 'like'): 0.25,\n",
       " ('like', 'spencer'): 0.11111111111111116,\n",
       " ('spencer', 's'): 0.16666666666666663,\n",
       " ...}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seventh(text = text1):\n",
    "    #words = set(list_of_words_from_text(text))\n",
    "    #print(words)\n",
    "    dic = {}\n",
    "    #dic = {(a,b) : (1-nltk.jaccard_distance(set(list(a)),set(list(b)))) for (a,b) in combinations(words,2)}\n",
    "    words = list(filter(None, [word.lower().translate(str.maketrans('', '', punctuation))\n",
    "                       for line in text\n",
    "                       for word in line.split()\n",
    "                       ]))\n",
    "    for i in range(len(words)-1):\n",
    "        a = words[i]\n",
    "        b = words[i+1]\n",
    "        if((a,b) not in dic.keys() and (b,a) not in dic.keys() and a != b): \n",
    "            dist = 1-nltk.jaccard_distance(set([c for c in a]), set([c for c in b]))\n",
    "            dic.update({(a,b):dist})\n",
    "    #for pair in combinations(words,2):\n",
    "    #    print(pair)\n",
    "        \n",
    "    #for a in words:\n",
    "    #    for b in words:\n",
    "    #        if((a,b) not in dic.keys() and (b,a) not in dic.keys() and a != b): \n",
    "    #            dist = 1-nltk.jaccard_distance(set([c for c in a]), set([c for c in b]))\n",
    "    #            dic.update({(a,b):dist})\n",
    "    return dic\n",
    "    #for pair in combinations(words,2):\n",
    "    #    dist = 1 - nltk.jaccard_distance(pair[0],pair[1])\n",
    "    #    dic.update({pair:dist})\n",
    "    #    if(dist > maximum):\n",
    "    #        maximum = dist\n",
    "\n",
    "dc = seventh()\n",
    "dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f717cb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1888d2f",
   "metadata": {},
   "source": [
    "#### 8) For two words v and w, let relative edit distance be the Levensthein distance between v and w divided by the sum of lengths v and w. Find two different words in text2 with minimal relative edit distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e287aec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Min K., Kao MY., Zhu H. (2009) The Closest Pair Problem under the Hamming Metric\n",
    "# they say that it's hard to do it much faster and have number close to O(N^2)\n",
    "def eighth(text=text2):\n",
    "    words = set(list_of_words_from_text(text))\n",
    "    minimal = 1\n",
    "    dic ={}\n",
    "    for (a,b) in combinations(words,2):\n",
    "        if((a,b) not in dic.keys() and (b,a) not in dic.keys() and a != b):\n",
    "            if 1.0/(len(a)+len(b)) > minimal : continue\n",
    "            else:\n",
    "                dist = relative_edit_dist(a,b)\n",
    "                dic.update({(a,b)})\n",
    "                if dist < minimal:\n",
    "                    minimal = dist\n",
    "    return minimal\n",
    "    #for a in words:\n",
    "    #    for b in words:\n",
    "    #        if((a,b) not in dic.keys() and (b,a) not in dic.keys() and a != b):\n",
    "    #            dist = relative_edit_dist(a,b)\n",
    "    #            dic.update({(a,b):dist})\n",
    "    #            if dist < minimal:\n",
    "    #                minimal = dist\n",
    "    #return minimal\n",
    "eighth()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7e153e",
   "metadata": {},
   "source": [
    "#### 9) For a given bitstring b and a natural number r list all bitstrings bâ€™, such that the Hamming distance between b and bâ€™ is equal n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "fcc0993c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0111', '1011', '1101', '1110'}"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ninth(bitstring, n):\n",
    "    if len(bitstring) < n: raise ValueError(\"too short\")\n",
    "    str_set =[]\n",
    "    temp = [0*i if (i < len(bitstring)-n) else 1 for i in range(len(bitstring)) ]\n",
    "    #joinedlist = temp + ones\n",
    "    perm = permutations(temp)\n",
    "    for permutation in perm:\n",
    "        #print(list(permutation))\n",
    "        temp_str = \"\"\n",
    "        for i in range(len(bitstring)):\n",
    "            temp_str += str((int(bitstring[i]) + permutation[i] )%2)\n",
    "        str_set.append(temp_str)\n",
    "    return set(str_set)\n",
    "ninth(\"0000\",3)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a46a9d1",
   "metadata": {},
   "source": [
    "####  10) Construct a function that for a given string and a natural number k returns a set of all its k-shingles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "b61c8cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('czy', 'ja', 'wiem'),\n",
       " ('czy', 'to', 'najszybciej'),\n",
       " ('ja', 'wiem', 'czy'),\n",
       " ('wiem', 'czy', 'to')}"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tenth(sentence,k=2):\n",
    "    split = tokenizer.tokenize(sentence.lower())\n",
    "    if len(split) < k: raise ValueError('too short for specified shingle length of {}'.format(k))\n",
    "    \n",
    "    return set(nltk.ngrams(split,k))\n",
    "tenth(\"czy ja wiem, czy to najszybciej?\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c75446",
   "metadata": {},
   "source": [
    "#### 11) Generate a set S of n random bitstrings of length 100. Find minx;y2S sha-1(xky), where xky denotes concatenation of bitstrings x and y. Estimate, what is the maximal n for this task that can be handled by your computer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "2040ec1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(b'\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00',\n",
       "  b'\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00'): 1461491702522384226984090568288406928933418515457}"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eleventh(n,length=100):\n",
    "    # one takes 100 bytes, i got 16gb\n",
    "    # 2^len are all possible strings of len 100\n",
    "    # we can take 2^4*10^7 then approx 2^4 *2 ^20 * 2^3 approx 2^27\n",
    "    mini = sys.maxsize\n",
    "    s = set()\n",
    "    for i in range(n):\n",
    "        s.add((np.random.randint(2, size=(length,))).tobytes())\n",
    "    # making pairs\n",
    "    x =\"\"\n",
    "    y =\"\"\n",
    "    for (a,b) in combinations(s,2):\n",
    "        tmp = int(hashlib.sha1(a+b).hexdigest(),16)\n",
    "        if(mini < tmp):\n",
    "            mini = tmp\n",
    "            x = a\n",
    "            y = b\n",
    "    return {(x,y):mini}\n",
    "eleventh(pow(2,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d7fa27",
   "metadata": {},
   "source": [
    "#### 12) Let S1; S2; S3 be the sets of all words shorter than 8 letters from text1, text2, text3, respectively. Compute signatures for S1; S2; S3 represented by 100 minhashes and then estimate Jaccard similarity between each pair of S1; S2; S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "b79710b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1->2': 'real=0.293270395 , minhash=0.295600000',\n",
       " '1->3': 'real=0.146474854 , minhash=0.148600000',\n",
       " '2->3': 'real=0.212060302 , minhash=0.207000000'}"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mmh3.hash_bytes('foo')  # 128-bit value as bytes\n",
    "def min_of_hash(secik, seed):\n",
    "    mini = sys.maxsize\n",
    "    for a in secik:\n",
    "        hashs= mmh3.hash(a.encode('utf8'),seed)\n",
    "        #print(hashs)\n",
    "        if hashs < mini :\n",
    "            mini = hashs\n",
    "    return mini\n",
    "    \n",
    "\n",
    "def signatures12(text_a = text1, text_b = text2, text_c = text3, shorten = 8,hashnum = 100,shingles = 4):\n",
    "    list_of_seeds = [random.randint(0,sys.maxsize) + i * 0 for i in range(hashnum)]\n",
    "    words1 = read_words_cut(text_a)\n",
    "    words1 = k_shingle_set(words1,shingles )\n",
    "    words2 = read_words_cut(text_b)\n",
    "    words2 = k_shingle_set(words2,shingles )\n",
    "    words3 = read_words_cut(text_c)\n",
    "    words3 = k_shingle_set(words3,shingles )\n",
    "    real1_2 = 1 - nltk.jaccard_distance(words1,words2)\n",
    "    real1_3 = 1 - nltk.jaccard_distance(words1,words3)\n",
    "    real2_3 = 1 - nltk.jaccard_distance(words2,words3)\n",
    "    ls1 = []\n",
    "    ls2 = []\n",
    "    ls3 = []\n",
    "    N = min(len(words1),len(words2),len(words3))\n",
    "    for seed in list_of_seeds:\n",
    "        ls1.append(min_of_hash(words1,seed)%(2*N))\n",
    "        ls2.append(min_of_hash(words2,seed)%(2*N))\n",
    "        ls3.append(min_of_hash(words3,seed)%(2*N))\n",
    "    one_two = 0\n",
    "    one_three =0\n",
    "    two_three = 0\n",
    "    for i in range(len(ls1)):\n",
    "        one_two += int(ls1[i]==ls2[i])\n",
    "        one_three += int(ls1[i]==ls3[i])\n",
    "        two_three += int(ls2[i]==ls3[i])\n",
    "    return {\"1->2\" : \"real=\"+ f'{real1_2:.9f}' +\" , minhash=\" + f'{1.0*one_two/len(ls1):.9f}',\n",
    "            \"1->3\" : \"real=\"+f'{real1_3:.9f}' +\" , minhash=\" + f'{1.0*one_three/len(ls1):.9f}',\n",
    "            \"2->3\" : \"real=\"+f'{real2_3:.9f}' +\" , minhash=\" + f'{1.0*two_three/len(ls1):.9f}'}\n",
    "\n",
    "signatures12(hashnum = 5000, shingles = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffbe0a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844d45ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f11210d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d6edcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
